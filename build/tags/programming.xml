<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><title>SIGUSR2: tagged programming</title><link href="http://sigusr2.net/tags/programming.xml" rel="self" /><link href="http://sigusr2.net/"/><updated>2011-04-18T07:30:47Z</updated><author><name>Andrew Gwozdziewycz</name></author><id>md5:ec2c8804a8dac6a866e1a43cfce32fc1</id><entry><title>unipoint-mode: A minor-mode for Mathematical Unicode</title><link href="http://sigusr2.net/2010/Nov/04/unipoint-minor-mode-for-mathematical-unicode.html"/><id>md5:51002542f1e8ffaff3c09d17aea84c99</id><updated>2010-11-04T00:00:00Z</updated><content type="html"><![CDATA[<p><span class="preamble">Last week I <a href="http://fold.sigusr2.net/2010/10/sir-please-step-away-from-the-asr-33-acm-queue.html">reblogged</a> Poul-Henning Kamp's article <a href="http://queue.acm.org/detail.cfm?id=1871406">"Sir, Please Step away from the ASR-33"</a>.</span></p>

<p>If you haven't read it, do so. It has an interesting outlook on what it means to write code, and asks the question, "why the hell are we stuck in the 60s?" His basic premise is that we're married to ASCII, despite having all sorts of other characters at our disposal via unicode, which provides much clearer and concise possibilities for syntax. He also goes on about how none of us have monochromatic screens, so color could play some role in what our programs mean, as well as arguing that the vertical nature of code is unjustified, since we have column editing.</p>

<p>If you ask me, his other arguments are crap, but the unicode argument is completely valid. There's just one problem, have you ever tried to type unicode? It's a mess. In Emacs, to type &rarr;, it's <code class="inline">C-x 8 RET RIGHTWARDS ARROW RET</code>, or if you know the hex value, <code class="inline">C-x 8 RET 2192 RET</code>. Sure, there are <a href="http://xahlee.org/emacs/emacs_n_unicode.html">other options</a>, but most editors aren't nearly as flexible to customize as Emacs is, so who knows what it's like outside of it&mdash;I can't imagine it's any better.</p>

<p>But, when I commented on the fact that "typing unicode sucks," I was greeted by an email from a friend of mine:</p>

<blockquote><a href="http://racket-lang.org/">DrRacket</a> lets you enter common Unicode characters by typing their
LaTeX name followed by control-\. For example, you type &isin; by typing
"\", "i", "n", "control-\". It's easy enough that I use Unicode in my
code all the time.

I bet it wouldn't be too hard to hack something like this feature into Emacs.<span class="closequote"></span>
<cite>&mdash;<a href="http://users.eecs.northwestern.edu/~clk800/">Casey Klein</a></cite>
</blockquote>

<p>This got me thinking about the problem, and in 20 minutes I had <code class="inline">C-\</code> bound to a function that would read a TeX symbol name from the minibuffer and looked up the unicode character in a table then spit it out. I then began to go on a typing spree, seeing how easy it actually was to use. Of course, this wasn't exactly what Casey had described, but it was close enough, and made it way better than before.</p>

<p>It actually had some great features of it's own, too. For one, it had TAB completion, to cut down on typing. DrRacket didn't have that.</p>

<p>I told Casey about my efforts and mentioned TAB completion and how "this isn't all that bad." Casey was intrigued by the TAB completion, and I wanted to see if <code class="inline">C-\</code> after typing \in was better&mdash;it is, but there's still value some value in the prefixed entry.</p>

<p>So now I had a function bound that would do the right thing. If point was at the end of a word boundary with a \ to the left of it (the word), it'd attempt to convert the sequence to a symbol, otherwise it'd be left alone. If you were in empty space or the completion failed, you'd be asked for the symbol as if you were going the prefix route.</p>

<p>This all worked wonderfully well, and our conversation about why I don't like entering unicode went on; he was still convinced it was woefully easy. I, too, was beginning to see that it's not as painful with the proper tools as I originally thought.</p>

<p>The 43 message thread of back and forth sparked inspiration in him around the idea of completion for DrRacket. His idea was simple, hit <code class="inline">C-\</code> and it'd attempt to complete whatever was before it, or output the longest subsequence of the symbols, prefixed with whatever you typed, in the lookup table. In other words, if you had "\sub" it'd complete to "\subset" because both "\subset" and "\subseteq" are in the lookup table. Hit <code class="inline">C-\</code> again and the substitution to &isin; takes place.</p>

<p>I couldn't help but be inspired to add that behavior to what I had now dubbed <a href="http://github.com/apgwoz/unipoint">unipoint</a>. His <a href="https://github.com/plt/racket/commit/bd0ebc7511c7b66dfdd0b24d68dbe27077a9a7dd">changes</a> went into DrRacket</a>. We both found solace in the fact that typing symbols was a bit easier than it was before.</p>

<p>I'm a lot less scared now to see symbols in code, and certainly advocating for them, so long as they stay mathematical in nature. I can't help but feel, though, that most people see this still as a problem. It's because of this that I recorded a quick screencast tutorial (no audio) of how unipoint is used. Hopefully it does its job in showing that it doesn't have to be so painful, and Kamp's dream will eventually become a reality.</p>

<iframe src="http://player.vimeo.com/video/16461894" width="400" height="300" frameborder="0"></iframe>
]]></content></entry>
<entry><title>The Failure of Bayesian Filtering For Automated Screening of Med School Applications</title><link href="http://sigusr2.net/2008/Sep/18/bayesian-failure.html"/><id>md5:f10c2840e1d10debae66e6dfe782f90e</id><updated>2008-09-18T22:20:00Z</updated><content type="html"><![CDATA[<p>
<span class="preamble">OK, I'll admit it. I <em>failed</em>. I failed to apply a simple Bayes net to a simple problem, which humans can do with a little bit of reading and some coffee.</span>
</p>

<p>At my place of employment, we have a problem. Our solutions to this problem, right now, relies too much on time consuming human interaction and, certainly, too much on the knowledge gained from history. The problem is that of a multitude of pre-med students hoping to be granted the opportunity to come here for an interview regarding admission into the world renouned medical school. </p>

<p>It'd be both impractical and impossible to interview everyone, so there exists a rigorous screening process before interview decisions are made. We have some tools that automatically look through an application and parameterise it for easier searching. We also have some tools that automatically weed out applications that have extremely low <abbr title="Medical College Admission Test">MCAT</abbr>s and <abbr title="Grade Point Average">GPA</abbr>s. By and large, these processes get the number of applications the screening team must thoroughly inspect down to a manageable number of very competitive candidates, but the number is still too large.</p>

<p>Now, immediately you must be thinking, <q>Why can't they just change the parameters to eliminate more people?</q> And the answer is that we could, but what the screeners look for isn't just numeric. The screeners are looking at many different aspects of the candidates past. Many of the students accepted to the school weren't the top students in their class. They certainly weren't horrible students academically, but they had diverse experiences in school, or otherwise, that the professors of the school felt qualified them as being an outstanding addition to this premier institution.</p>

<p>As a programmmer, I see this as a fun and interesting challenge. A challenge where I can apply some "computer sciency" things to add just a touch of intelligence to the automated process and filter out those candidates that maybe are not as "rock star" as they appear in numbers. My first though was to use a <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier" title="Naive Bayes on WikiPedia">Naive Bayes classifier</a>.</p>

<p>A medical school applicant writes a lot. <a href="http://www.aamc.org" title="American Medical College Application Service">AMCAS</a> asks of students to submit information on past experiences (e.g. research, personal, travel, professional, community service), and for information about publications they've (co)authored, and a personal statement. In addition, applicants submit some other free form text to us directly. This data seems like a natural fit for training a classifier.</p>

<p>So, I go with it; I mention to my project manager that it might be possible to probabalistically predict the scores generated from the manual, time consuming screening process, using historic applicant entered data as a training corpus. As his eyes lit up, I explained the basic idea of a Naive Bayes classifier and said, <q>it's the basis of many SPAM filters.</q> This was enough to sell it to him, and to our immediate client, the Admissions office. I now had permission to play around and see how well I could predict the past.</p>

<p>From the database, I pulled out as much data as I possible could and build two buckets; successful and unsuccessful. Successful candidates had the property that they were all likely invited for interviews, and well unsuccessful weren't. I tried to make it roughly even, taking as many of the least successful candidates as I could to match as closely as possible to the number of successful candidates. I proceeded to write a basic Naive Bayes classifier, made it general enough that I could add and subtract some ideas easily, and setup a simple cross validation. I was set.</p>

<p>I couldn't have been more excited to see things pass the screen like:</p>

<pre><code>Run 0 -------------
Good: S 57 / F 17 = 0.773333
Bad: S 22 / F 49 = 0.319444
Run 1 -------------
Good: S 71 / F 3 = 0.960000
Bad: S 8 / F 63 = 0.125000
Run 2 -------------
Good: S 70 / F 4 = 0.946667
Bad: S 13 / F 58 = 0.194444
Run 3 -------------
Good: S 61 / F 13 = 0.826667
Bad: S 12 / F 59 = 0.180556</code></pre>

<p>But the results didn't look good to me. The first line "Good: ... " states that of the 74 attempts at classifying a known "successful" candidate only 57 were predicted correctly. Even worse, only 22 of the "unsuccessful" candidates were predicted successfully. Surely, this could be fixed, I thought. It's simply a matter of doing better smoothing, and eliminating the common words with high frequencies. </p>

<p>Still nothing. Stem the words? Worse. Eliminate more words? Worse. Smoothing? No better. Discouragement sets in.</p>

<p>I look into it a bit further and discover something interesting. All these essays, all these experiences sound exactly the same! They all talk about community service and late nights in the research labs. They all discuss the fact that for as long as they can remember they've wanted to become physicians. It's as if all the candidates are the same person.</p>

<p>They aren't. Unlike a note from my wife and an offer for viagra or cialis, their goals are the same, and their language is in the same domain.</p>

<p>Seems like I need another idea.</p>
]]></content></entry>

</feed>